{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880f767c-063e-41ab-b717-159c314557f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89286644-478c-45ea-b4c4-51357958de33",
   "metadata": {},
   "source": [
    "Carico il configuratore che tiene traccia di tutte le informazioni generali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ea09f7-d49a-4aff-bcd2-e1bf7e5c97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "with open(\"./config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5028948-9440-480c-8007-9bb486239ecc",
   "metadata": {},
   "source": [
    "# Download Dati\n",
    "\n",
    "In questa parte carico i `.csv` e gli organizzo in modo tale che siano ordinati per\n",
    "\n",
    "`(data, denominazione_regionale, denominazione_provincia)`\n",
    "\n",
    "Inoltre pulisco i vari datase poiché ci sono una serie di righe che presentano delle problematicità in termini di valori.\n",
    "\n",
    "La matrice di adiacenza generale è stata creata connettendo tutte le provincie di una determinata regione, in modo tale da mantenere una similitudine con il problema originario. Inoltre, per non distaccarsi troppo con la realtà, sono state inserite delle connessioni tra regioni dato che quest'ultime non sono isolate. In particolare sono state aggiunte connessioni tra tutti i capoluoghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b263e7-92d5-41c3-b938-4feab58d4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(config: yaml):\n",
    "    if os.path.exists(os.path.join(config['paths']['adj'],\"adj_totale.pkl\")):\n",
    "        with open(os.path.join(config['paths']['adj'],\"adj_totale.pkl\"), \"rb\") as f:\n",
    "            adj = pickle.load(f)\n",
    "    else:\n",
    "        codice_reg_prov = pd.read_csv(os.path.join(config['paths']['data'], \"codice_reg_prov.csv\"), index_col=0)\n",
    "        capoluoghi = {'Piemonte': \"Torino\", \n",
    "                      \"Valle d'Aosta\": \"Aosta\", \n",
    "                      'Lombardia': \"Milano\", \n",
    "                      'Trentino':\"Trento\", \n",
    "                      'Veneto':\"Venezia\",\n",
    "                      'Friuli Venezia Giulia': \"Trieste\", \n",
    "                      'Liguria': \"Genova\", \n",
    "                      'Emilia-Romagna': \"Bologna\", \n",
    "                      'Toscana': \"Firenze\",\n",
    "                      'Umbria':\"Perugia\", \n",
    "                      'Marche' : \"Ancona\", \n",
    "                      'Lazio': \"Roma\", \n",
    "                      'Abruzzo': \"L'Aquila\", \n",
    "                      'Molise': \"Campobasso\", \n",
    "                      'Campania': \"Napoli\",\n",
    "                      'Puglia': \"Bari\", \n",
    "                      'Basilicata': \"Potenza\", \n",
    "                      'Calabria': \"Catanzaro\", \n",
    "                      'Sicilia': \"Palermo\", \n",
    "                      'Sardegna': \"Cagliari\"}\n",
    "        adjs = {}\n",
    "        adj = np.zeros((len(codice_reg_prov), len(codice_reg_prov)))\n",
    "        i = 0\n",
    "        index_capoluoghi = []\n",
    "        for cod_reg in tqdm(codice_reg_prov.codice_regione.unique()):\n",
    "            n_prov = len(codice_reg_prov[codice_reg_prov.codice_regione == cod_reg])\n",
    "            adjs[cod_reg] = np.ones((n_prov, n_prov))\n",
    "            adj[i:i+n_prov, i:i+n_prov] = adjs[cod_reg]\n",
    "            \n",
    "    \n",
    "            tmp = codice_reg_prov[codice_reg_prov.codice_regione == cod_reg].reset_index(drop = True)\n",
    "            region = codice_reg_prov[codice_reg_prov.codice_regione == cod_reg].denominazione_regione.unique()[0]\n",
    "            ind = tmp[tmp.denominazione_provincia == capoluoghi[region]].index[0]\n",
    "            index_capoluoghi.append(i+ind)\n",
    "    \n",
    "            with open(os.path.join(config['paths']['adj'],f\"adj_{region}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(adj, f)\n",
    "                \n",
    "            i += n_prov\n",
    "        for ind_r in index_capoluoghi:\n",
    "            for ind_c in index_capoluoghi:\n",
    "                adj[ind_r, ind_c] = 1\n",
    "                \n",
    "        with open(os.path.join(config['paths']['adj'],\"adj_totale.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(adj, f)\n",
    "        \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1ab253-e1e1-447e-9627-ad5560fb24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dataset(df: pd, \n",
    "                 config:yaml):\n",
    "    # questa procedura serve solo ad inserire i dati sulle province mancanti\n",
    "    d = []\n",
    "    feat = df.shape[1]\n",
    "    codici = list(df.codice_provincia.unique())\n",
    "    codice_reg_prov = {tuple(x) for x in df[config['dataset']['col_categorical_prov']].values.tolist()}\n",
    "    codice_reg_prov = pd.DataFrame(codice_reg_prov, columns=config['dataset']['col_categorical_prov'])\n",
    "    j = 0\n",
    "    for data in tqdm(df.data.unique(), desc = \"filling\"):\n",
    "        tmp = df[df.data == data]\n",
    "        if len(tmp) == len(codici):\n",
    "            d.append(tmp)\n",
    "        else:\n",
    "            j+=1\n",
    "            # individue le province mancanti\n",
    "            codici_mancanti = [x for x in codici if x not in list(tmp.codice_provincia.unique())]\n",
    "            code_regioni_mancanti = [codice_reg_prov[codice_reg_prov.codice_provincia == x].codice_regione.values[0] for x in codici_mancanti]\n",
    "            tmp_mancante = pd.DataFrame(np.zeros((len(codici_mancanti),feat)), columns = df.columns)\n",
    "            \n",
    "            tmp_mancante.data = data\n",
    "            tmp_mancante.codice_provincia = codici_mancanti\n",
    "            tmp_mancante.codice_regione = code_regioni_mancanti\n",
    "            tmp = pd.concat((tmp, tmp_mancante))\n",
    "            if len(tmp) != len(codice_reg_prov):\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            d.append(tmp)\n",
    "    df = pd.concat(d)\n",
    "\n",
    "    bilancio = {}\n",
    "    for csv in os.listdir('./data/'):\n",
    "        if 'Bilancio demografico' in csv:\n",
    "            tmp = pd.read_csv(f\"./data/{csv}\")\n",
    "            tmp = tmp[tmp.Sesso == \"Totale\"][['Codice provincia', 'Popolazione al 1° gennaio']]\n",
    "            tmp.rename(columns={'Codice provincia': 'codice_provincia', \n",
    "                                'Popolazione al 1° gennaio': 'popolazione'}, inplace=True)\n",
    "            anno = int(csv.split(\" \")[-1][:4])\n",
    "            bilancio[anno] = tmp\n",
    "    merge = []\n",
    "    for anno in bilancio.keys():\n",
    "        if anno != 2022:\n",
    "            tmp = df[(df.data>pd.to_datetime(anno, format = '%Y'))&(df.data<pd.to_datetime(anno+1, format = '%Y'))]\n",
    "        else:\n",
    "            tmp = df[df.data>pd.to_datetime(anno, format = '%Y')]\n",
    "        bi = bilancio[anno]\n",
    "        bi = bi[bi.codice_provincia.isin(codice_reg_prov.codice_provincia.unique().tolist())]\n",
    "        tmp = pd.merge(tmp, bi, on = 'codice_provincia', how ='left')\n",
    "        merge.append(tmp)\n",
    "    \n",
    "    df = pd.concat(merge)    \n",
    "    # Qua ordino le colonne come voglio altrimenti avrei variabile numeriche dove sono quelle categoriche\n",
    "    df = df[config['dataset']['col_data'] + config['dataset']['col_numerical_prov']+ ['popolazione'] + config['dataset']['col_numerical_reg']+config['dataset']['col_categorical_prov']]\n",
    "    df = df.drop(columns=[\"denominazione_regione\", \"denominazione_provincia\"])\n",
    "    df = df.sort_values(by = config['dataset']['ordering']).drop_duplicates(config['dataset']['ordering']).reset_index(drop = True)\n",
    "    \n",
    "    y = df[['data','codice_provincia', 'codice_regione', 'nuovi_casi']]    \n",
    "    \n",
    "    ## normalization for each region\n",
    "    d = []\n",
    "    col = config['dataset']['numerical']\n",
    "    remaining = [x for x in list(df.columns) if (x not in col)&(x!='data')]\n",
    "    for cod in df.codice_regione.unique():\n",
    "        tmp = df[df.codice_regione==cod]        \n",
    "        index = int(len(tmp)*0.8)\n",
    "        for column in col:\n",
    "            min = tmp[column].values[:index].min()\n",
    "            max = tmp[column].values[:index].max()\n",
    "            tmp[column] = (tmp[column]-min)/(max-min)            \n",
    "        \n",
    "        d.append(tmp)\n",
    "        \n",
    "    tmp = pd.concat(d,0)\n",
    "    df = tmp.sort_values(by = config['dataset']['ordering']).reset_index(drop = True)  \n",
    "    return df, y\n",
    "    \n",
    "def get_dataset(config: yaml):\n",
    "    files = [\"covid\",\"target\", \"codice_reg_prov\", \"covid_province\", \"covid_regioni\"]\n",
    "    agg = \"aggregate\" if config['dataset']['aggregate'] else \"not_aggregate\"\n",
    "    exists = [os.path.exists(os.path.join(config['paths']['data'],\"aggregate\", f\"{file}.csv\")) for file in files]\n",
    "    \n",
    "    if  np.all(exists):\n",
    "        out = ()\n",
    "        for file in files:\n",
    "            tmp = pd.read_csv(os.path.join(config['paths']['data'], 'aggregate', f\"{file}.csv\"), index_col=0)\n",
    "            if \"data\" in list(tmp.columns):\n",
    "                tmp['data'] = pd.to_datetime(tmp['data'], format='%Y-%m-%d')\n",
    "            out += (tmp,)\n",
    "        return out[0], out[1]\n",
    "        \n",
    "    else:\n",
    "        ########## I create take the dataset for the provinces\n",
    "        if os.path.exists(os.path.join(config['paths']['data'],\"aggregate/covid_province.csv\")):\n",
    "            provincials = pd.read_csv(os.path.join(config['paths']['data'],\"covid_province.csv\"))\n",
    "            provincials['data'] = pd.to_datetime(provincials['data'], format='%Y-%m-%d')\n",
    "        else:\n",
    "            data = []\n",
    "            for csv in tqdm(os.listdir(config['paths']['dati-province']), desc = \"provincia\"):\n",
    "                if csv.split(\".\")[-1] == \"csv\":\n",
    "                    data.append(pd.read_csv(os.path.join(config['paths']['dati-province'],csv)))\n",
    "            \n",
    "            \n",
    "            data = pd.concat(data)\n",
    "            data.reset_index(drop = True, inplace=True)\n",
    "            data.data = data.data.apply(lambda x: x.split(\"T\")[0])\n",
    "            data.data = pd.to_datetime(data['data'], format='%Y-%m-%d')\n",
    "            \n",
    "            # Riduco il dataset con le variabili che servono\n",
    "            # Inoltre trasformo la conta totale dei positivi per ogni regione in nuovi positivi\n",
    "            data.rename(columns={'totale_casi': 'nuovi_casi'}, inplace=True)\n",
    "            provincials = data[config['dataset']['col_data'] + config['dataset']['col_categorical_prov'] + config['dataset']['col_numerical_prov']]\n",
    "            provincials = provincials.sort_values(by = config['dataset']['ordering'])\n",
    "            provincials = provincials.drop_duplicates()\n",
    "            provincials = provincials[-(provincials.codice_provincia>200)]\n",
    "            provincials = provincials[-provincials.codice_regione.isin([21,22])].reset_index(drop = True)\n",
    "            provincials.denominazione_regione = provincials.denominazione_regione.replace(['P.A. Bolzano', 'P.A. Trento'], \"Trentino\")\n",
    "            \n",
    "            tmp = provincials[['data','codice_provincia', 'nuovi_casi']]\n",
    "            tmp = tmp.groupby('codice_provincia').diff().dropna().drop(columns = \"data\")\n",
    "            # Non ha senso avere questi valori\n",
    "            tmp = tmp[-tmp.nuovi_casi<0]\n",
    "            \n",
    "            #faccio un merge sugli indici\n",
    "            provincials = pd.merge(provincials.drop(columns = \"nuovi_casi\"),tmp, left_index=True, right_index=True)\n",
    "            provincials = provincials.reset_index(drop=True)\n",
    "            provincials.to_csv(os.path.join(config['paths']['data'],\"aggregate/covid_province.csv\"))\n",
    "\n",
    "                    \n",
    "        if os.path.exists(os.path.join(config['paths']['data'], \"aggregate/covid_regioni.csv\")):\n",
    "            regions = pd.read_csv(os.path.join(config['paths']['data'],\"aggregate/covid_regioni.csv\"))\n",
    "            regions['data'] = pd.to_datetime(regions['data'], format='%Y-%m-%d')\n",
    "        else:\n",
    "            ########## I create take the dataset for the regions\n",
    "            data = []\n",
    "            for csv in tqdm(os.listdir(config['paths']['dati-regioni']), desc = \"regione\"):\n",
    "                if csv.split(\".\")[-1] == \"csv\":\n",
    "                    data.append(pd.read_csv(os.path.join(config['paths']['dati-regioni'],csv)))\n",
    "            data = pd.concat(data).reset_index(drop = True)\n",
    "            \n",
    "            # Trento\n",
    "            data.codice_regione = data.codice_regione.replace(21, 4)\n",
    "            # Bolzano\n",
    "            data.codice_regione = data.codice_regione.replace(22, 4)\n",
    "            data.denominazione_regione = data.denominazione_regione.replace(['P.A. Bolzano', 'P.A. Trento'], \"Trentino\")\n",
    "            \n",
    "            data.data = data.data.apply(lambda x: x.split(\"T\")[0])\n",
    "            data.data = pd.to_datetime(data['data'], format='%Y-%m-%d')\n",
    "            regions = data[config['dataset']['col_data'] + config['dataset']['col_categorical_reg'] + config['dataset']['col_numerical_reg']]\n",
    "            regions = regions.drop_duplicates().reset_index(drop = True)\n",
    "            regions.to_csv(os.path.join(config['paths']['data'],\"aggregate/covid_regioni.csv\"))\n",
    "                    \n",
    "        # Creo il dataset completo \n",
    "        if os.path.exists(os.path.join(config['paths']['data'],\"aggregate/covid.csv\")):\n",
    "            df = pd.read_csv(os.path.join(config['paths']['data'], \"aggregate/covid.csv\"), index_col=0)\n",
    "            df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
    "            y = pd.read_csv(os.path.join(config['paths']['data'], \"aggregate/target.csv\"), index_col=0)\n",
    "            y['data'] = pd.to_datetime(y['data'], format='%Y-%m-%d')\n",
    "        else:\n",
    "            df = pd.merge(provincials, regions, \n",
    "                          how=\"left\",  \n",
    "                          on = config['dataset']['merge']) \n",
    "            df = df.sort_values(by = config['dataset']['ordering']).drop_duplicates(['data','codice_provincia']).reset_index(drop = True)\n",
    "            df = df[config['dataset']['col_data'] + config['dataset']['col_numerical_prov']+config['dataset']['col_numerical_reg']+config['dataset']['col_categorical_prov']]\n",
    "\n",
    "            codice_reg_prov = {tuple(x) for x in df[config['dataset']['col_categorical_prov']].values.tolist()}\n",
    "            codice_reg_prov = pd.DataFrame(codice_reg_prov, columns=config['dataset']['col_categorical_prov'])\n",
    "            df, y = fill_dataset(df, config)\n",
    "            y.to_csv(os.path.join(config['paths']['data'],f\"aggregate/target.csv\"))\n",
    "            if ~config['dataset']['aggregate']:\n",
    "                for codice in df.codice_regione.unique():\n",
    "                    tmp = df[df.codice_regione == codice].reset_index(drop = True)\n",
    "                    tmp.to_csv(os.path.join(config['paths']['data'],f\"not_aggregate/covid_{codice}.csv\"))\n",
    "                    y.to_csv(os.path.join(config['paths']['data'],f\"not_aggregate/target_{codice}.csv\"))\n",
    "            codice_reg_prov.to_csv(os.path.join(config['paths']['data'],\"aggregate/codice_reg_prov.csv\"))\n",
    "            df.to_csv(os.path.join(config['paths']['data'],\"aggregate/covid.csv\"))\n",
    "            \n",
    "    return df, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c10aa6-2587-4efd-80c2-fdf9442bbb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = get_dataset(config)\n",
    "adj = get_adj(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7d157b-cf41-4236-99c3-335cceca7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class covid_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 df:pd,\n",
    "                 y:pd, \n",
    "                 past_step:int, \n",
    "                 future_step:int):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            df (pandas.Dataframe): Path to the csv file with annotations.\n",
    "            past_step (int): previous step to look back\n",
    "            future_step (int): future step to look for\n",
    "        \"\"\"\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        n = len(df.codice_regione.unique())\n",
    "        date = df.data.unique()\n",
    "        date.sort()\n",
    "        n_provincie = len(df.codice_provincia.unique())\n",
    "        start = 0\n",
    "        dt = np.diff(date[:past_step+future_step]) == np.timedelta64(1, 'D')\n",
    "        while any(not x for x in dt):\n",
    "            start +=1\n",
    "            dt = np.diff(date[start:past_step+future_step+ start]) == np.timedelta64(1, 'D')\n",
    "               \n",
    "        for i in tqdm(range(start, len(date)-future_step-past_step-1)):\n",
    "            if date[i+past_step+future_step]-date[i+past_step+future_step-1] == np.timedelta64(1, 'D'): \n",
    "                tmp_x = df[df.data.isin(date[i:i+past_step])].drop(columns = \"data\").values\n",
    "                tmp_y = y[df.data.isin(date[i+past_step:i+past_step+future_step])].nuovi_casi.values\n",
    "                \n",
    "                #print(tmp_x.shape)\n",
    "                if (len(tmp_x) == n_provincie*past_step) & ((len(tmp_y) == n_provincie*future_step)):\n",
    "                    tmp_y = tmp_y.reshape(future_step, -1)\n",
    "                    self.x.append(tmp_x)\n",
    "                    self.y.append(tmp_y)\n",
    "                else:\n",
    "                    print(len(tmp_x))\n",
    "            else:\n",
    "                i += past_step+future_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45083164-829f-4032-9a4c-c90dabd06f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1280/1280 [00:04<00:00, 262.79it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 616.52it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 690.61it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 494.46it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 616.26it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 635.83it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 483.40it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 507.10it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 541.99it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:03<00:00, 409.61it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 646.22it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 607.87it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 453.67it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 701.01it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 708.37it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 621.37it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 650.37it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 541.45it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 667.01it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:02<00:00, 630.68it/s]\n",
      "100%|██████████████████████████████████████| 1280/1280 [00:01<00:00, 702.46it/s]\n"
     ]
    }
   ],
   "source": [
    "past_step = config['setting']['past_step']\n",
    "future_step = config['setting']['future_step']\n",
    "\n",
    "if not os.path.exists(os.path.join(config['paths']['data'],f\"aggregate/dataset{past_step}_{future_step}.pt\")):\n",
    "    \n",
    "    dataset = covid_dataset(df,\n",
    "                            y,\n",
    "                            past_step=past_step, \n",
    "                            future_step=future_step)\n",
    "    torch.save(dataset, os.path.join(config['paths']['data'],f\"aggregate/dataset{past_step}_{future_step}.pt\"))\n",
    "\n",
    "for regione in df.codice_regione.unique():\n",
    "    if not os.path.exists(os.path.join(config['paths']['data'],f\"not_aggregate/dataset{regione}_{past_step}_{future_step}.pt\")):\n",
    "        dataset = covid_dataset(df[df.codice_regione == regione],\n",
    "                                y[y.codice_regione == regione],\n",
    "                                past_step=past_step, \n",
    "                                future_step=future_step)\n",
    "        torch.save(dataset, os.path.join(config['paths']['data'],f\"not_aggregate/dataset{regione}_{past_step}_{future_step}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7c110-9aef-4807-b280-8f1f6ac09e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for codice in df.codice_regione.unique()[:5]:    \n",
    "    plt.plot(df[df.codice_regione == codice].data.unique(), \n",
    "             df[df.codice_regione == codice].groupby('data').sum().nuovi_casi,'p', \n",
    "             label = codice)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
