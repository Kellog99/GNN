{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880f767c-063e-41ab-b717-159c314557f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "import pickle\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "from data import dataset\n",
    "from trainer import Trainer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89286644-478c-45ea-b4c4-51357958de33",
   "metadata": {},
   "source": [
    "Carico il configuratore che tiene traccia di tutte le informazioni generali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ea09f7-d49a-4aff-bcd2-e1bf7e5c97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/andrea/Scrivania/Tesi/leonardo/config_env.yaml\", 'r') as f:\n",
    "    config_env = yaml.safe_load(f)\n",
    "id_model = \"GLSTMseq2seq\"\n",
    "with open(os.path.join(config_env['paths']['config'], f\"{id_model}.yaml\"), 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "config.update(config_env)\n",
    "\n",
    "if 'epochs' in config_env['setting'].keys():\n",
    "    config['training']['epochs'] = config_env['setting']['epochs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7d157b-cf41-4236-99c3-335cceca7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30, 107, 21])\n",
      "torch.Size([4, 50, 107, 7])\n",
      "torch.Size([4, 107, 50])\n"
     ]
    }
   ],
   "source": [
    "past_step = config['setting']['past_step']\n",
    "future_step = config['setting']['future_step']\n",
    "batch_size= 4#config['setting']['batch_size']\n",
    "\n",
    "with open(os.path.join(config['paths']['data'], config['setting']['dataset'], f\"{past_step}_{future_step}.pkl\"), 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "len_train = int(len(dataset)*0.75)\n",
    "len_val = len(dataset)-len_train\n",
    "df_train, df_val = torch.utils.data.random_split(dataset=dataset, lengths = [len_train, len_val])\n",
    "dl_train = DataLoader(dataset=df_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(dataset=df_val, batch_size=batch_size, shuffle=True)\n",
    "x_past, x_fut, y, adj = next(iter(dl_train))\n",
    "config['setting']['in_feat_past'] = x_past.shape[-1]\n",
    "config['setting']['in_feat_future'] = x_fut.shape[-1]\n",
    "print(x_past.shape)\n",
    "print(x_fut.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b5eaa1-a59d-41a7-8960-cab807a39903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class embedding_layer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 categorical:list,\n",
    "                 dim_categorical:int):\n",
    "        \n",
    "        super(embedding_layer, self).__init__()\n",
    "        self.embedding = nn.ModuleList([nn.Embedding(categorical[i], dim_categorical) for i in range(len(categorical))])\n",
    "    def forward(self, x):\n",
    "    \n",
    "        out = 0.0\n",
    "        for i in range(len(self.embedding)):\n",
    "            out += self.embedding[i](x[:,:,:, i])    \n",
    "        return out.float()\n",
    "        \n",
    "class pre_processing(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feat:int, \n",
    "                 out_feat:int, \n",
    "                 dropout:float):\n",
    "        \n",
    "        super(pre_processing, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Dropout(dropout),\n",
    "                                    nn.Linear(in_features = in_feat, out_features = 128),\n",
    "                                    nn.ReLU(), \n",
    "                                    nn.Linear(in_features = 128, out_features = 128),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(in_features = 128, out_features = out_feat, bias=False))\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        return self.linear(x.float())\n",
    "\n",
    "class my_gcnn(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 dim_hidden_emb:int = 128):\n",
    "        super(my_gcnn, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.lin = nn.Linear(in_channels, \n",
    "                             out_channels, \n",
    "                             bias = False)\n",
    "        self.emb = nn.Linear(in_features = in_channels, \n",
    "                             out_features = dim_hidden_emb, \n",
    "                             bias = False)\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                x0: tuple) -> torch.tensor:\n",
    "        \n",
    "        x, A = x0   \n",
    "        x_emb = self.emb(x)\n",
    "        pi = torch.einsum('bdjk,bdik->bdij', x_emb, x_emb)\n",
    "\n",
    "        # Apply the mask to fill values in the input tensor\n",
    "        # sigmoid(Pi*X*W)\n",
    "        pi = F.softmax(pi.masked_fill(A == 0., -float('infinity')), -1)\n",
    "        x = torch.einsum('bpik,bpkj->bpij', pi, x)\n",
    "        x = F.sigmoid(self.lin(x))\n",
    "        \n",
    "        return (x, A)\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM weights and biases\n",
    "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_g = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        \n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = torch.cat((x, h_prev), dim=-1)\n",
    "\n",
    "        # Compute the input, forget, output, and cell gates\n",
    "        i = torch.sigmoid(self.W_i(combined))\n",
    "        f = torch.sigmoid(self.W_f(combined))\n",
    "        o = torch.sigmoid(self.W_o(combined))\n",
    "        g = torch.tanh(self.W_g(combined))\n",
    "\n",
    "        # Update the cell state and hidden state\n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c\n",
    "        \n",
    "        \n",
    "class GLSTMseq2seq(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_feat_past:int, \n",
    "                 in_feat_fut:int, \n",
    "                 past: int, \n",
    "                 future: int,\n",
    "                 categorical_past:list,\n",
    "                 categorical_future:list,\n",
    "                 device, \n",
    "                 out_preprocess:int = 128, \n",
    "                 dropout: float = 0.1, \n",
    "                 dim_categorical_past:int = 64, \n",
    "                 dim_categorical_future:int = 128, \n",
    "                 concat:bool = True,\n",
    "                 num_layer_gnn_past:int = 1,\n",
    "                 num_layer_gnn_future:int = 1,\n",
    "                 out_gnn: int = 128, \n",
    "                 hidden_gnn:int = 256,\n",
    "                 hidden_lstm: int = 128, \n",
    "                 hidden_propagation:int = 128):\n",
    "        \n",
    "        super(GLSTMseq2seq, self).__init__()\n",
    "        \n",
    "        self.in_feat_past = in_feat_past         # numero di features di ogni nodo prima del primo GAT        \n",
    "        self.in_feat_past = in_feat_fut\n",
    "        self.past = past \n",
    "        self.future = future\n",
    "        self.hidden_gnn = hidden_gnn\n",
    "        self.hidden_lstm = hidden_lstm\n",
    "        self.device = device\n",
    "        self.categorical_past = categorical_past\n",
    "        self.categorical_fut = categorical_future\n",
    "        \n",
    "        \n",
    "        ########## PREPROCESSING PART #############        \n",
    "        self.embedding_past = embedding_layer(categorical = categorical_past,\n",
    "                                            dim_categorical = dim_categorical_past)\n",
    "        if len(categorical_future)>0:\n",
    "            self.embedding_future = embedding_layer(categorical = categorical_future,\n",
    "                                                    dim_categorical = dim_categorical_future)\n",
    "        \n",
    "        in_feat_preprocessing_past = in_feat_past + dim_categorical_past - len(categorical_past)\n",
    "        self.pre_processing_past = pre_processing(in_feat = in_feat_preprocessing_past, \n",
    "                                                 out_feat = out_preprocess, \n",
    "                                                 dropout = dropout)\n",
    "        in_feat_preprocessing_fut = in_feat_fut + dim_categorical_future - len(categorical_future)\n",
    "        self.pre_processing_fut = pre_processing(in_feat = in_feat_preprocessing_fut, \n",
    "                                                 out_feat = out_preprocess, \n",
    "                                                 dropout = dropout)\n",
    "        ########## GNN ############# \n",
    "        ##### past\n",
    "        layers = []\n",
    "        for i in range(num_layer_gnn_past):\n",
    "            layers.append(my_gcnn(in_channels = out_preprocess if i == 0 else hidden_gnn, \n",
    "                                 out_channels = out_gnn if i == num_layer_gnn_past-1 else hidden_gnn))            \n",
    "        self.gnn_past = nn.Sequential(*layers)\n",
    "\n",
    "        ##### future\n",
    "        layers = []\n",
    "        for i in range(num_layer_gnn_future):\n",
    "            layers.append(my_gcnn(in_channels = out_preprocess if i == 0 else hidden_gnn, \n",
    "                                 out_channels = out_gnn if i == num_layer_gnn_future-1 else hidden_gnn))            \n",
    "        self.gnn_future = nn.Sequential(*layers)\n",
    "\n",
    "        ######### LSTM ################\n",
    "        # sia l'embedding del passato che quello del futuro hanno la stessa dimensionalità\n",
    "        # quindi poso usare lo stesso lstm e prendere come output gli ultimi `fut_step` \n",
    "        self.lstm = LSTMCell(input_size = out_gnn, \n",
    "                             hidden_size = hidden_lstm)\n",
    "        self.decoding = nn.Sequential(nn.Linear(in_features = hidden_lstm, \n",
    "                                                out_features = hidden_propagation), \n",
    "                                      nn.ReLU(), \n",
    "                                      nn.Linear(in_features = hidden_propagation,\n",
    "                                                out_features = hidden_propagation),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(in_features = hidden_propagation,\n",
    "                                                out_features = 1))\n",
    "               \n",
    "    def forward(self, x_past, x_fut, adj):        \n",
    "        ########### pre-processing dei dati ##########\n",
    "        ##### past \n",
    "        emb = self.embedding_past(x_past[:,:,:,-len(self.categorical_past):].int())\n",
    "        x_past = torch.cat((x_past[:,:,:,:-len(self.categorical_past)], emb), -1)\n",
    "        x_past = self.pre_processing_past(x_past)\n",
    "        \n",
    "        ##### future\n",
    "        if len(self.categorical_fut)>0:\n",
    "            emb = self.embedding_future(x_fut[:,:,:,-len(self.categorical_fut):].int())\n",
    "            x_fut = torch.cat((x_fut[:,:,:,:-len(self.categorical_fut)], emb), -1)    \n",
    "        x_fut = self.pre_processing_fut(x_fut)\n",
    "        \n",
    "        ########## GNN processing ######################\n",
    "        x_past, _ = self.gnn_past((x_past, adj))\n",
    "        x_fut, _ = self.gnn_future((x_fut, adj))\n",
    "\n",
    "        ########## LSTM part ###########################\n",
    "        x_lstm = torch.cat((x_past, x_fut), 1)\n",
    "        batch_size, seq_len, nodes, features = x_lstm.size()\n",
    "        h, c = [torch.zeros(batch_size, nodes, self.hidden_lstm).to(x_past.device)] * 2\n",
    "        out = []\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.lstm(x_lstm[:, t], (h, c)) \n",
    "            if t >= self.past:\n",
    "                out.append(self.decoding(c))\n",
    "        out = torch.cat(out, -1)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cpu\")       \n",
    "model = GLSTMseq2seq(in_feat_past = config['setting']['in_feat_past'],\n",
    "                        in_feat_fut = config['setting']['in_feat_future'],\n",
    "                        past = past_step,\n",
    "                        future = future_step,\n",
    "                        categorical_past = config['categorical'][config['setting']['dataset']]['past'],\n",
    "                        categorical_future = config['categorical'][config['setting']['dataset']]['future'],\n",
    "                        device = device).to(device)\n",
    "yh = model(x_past.to(model.device),x_fut.to(model.device), adj[0].to(model.device))\n",
    "yh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722b30e-158b-4a42-913d-f56aafcebf5b",
   "metadata": {},
   "source": [
    "## Loss of the model\n",
    "As always, the model must perform well on average on all points. \n",
    "$$\n",
    "\\frac{1}{n|T|}\\sum_{i=1}^n\\sum_{j}^{|T|}\\left|f\\left(t_j,x_{i,t_j}\\right)-\\hat{f}\\left(t_j,x_{i,t_j}\\right)\\right|\n",
    "$$\n",
    "In addition, it is important that the model did not have, at worst, too high an error.\n",
    "$$\n",
    "\\left(\\int_X\\left(\\int_T|f(t,x)-\\hat{f}(t,x)|\\,dt\\right)^p\\,dx\\right)^{\\frac{1}{p}}\\to\\max_{x\\in X}\\int_T|f(t,x)-\\hat{f}(t,x)|\\,dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de605738-eb49-43f1-aa22-5444ffa41700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = GLSTMseq2seq(in_feat_past = config['setting']['in_feat_past'],\n",
    "                        in_feat_fut = config['setting']['in_feat_future'],\n",
    "                        past = past_step,\n",
    "                        future = future_step,\n",
    "                        categorical_past = config['categorical'][config['setting']['dataset']]['past'],\n",
    "                        categorical_future = config['categorical'][config['setting']['dataset']]['future'],\n",
    "                        device = device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr = 1e-3, \n",
    "                       weight_decay=2e-4)\n",
    "def linfty(y, yh, gamma = 1.0, alpha=1e-1, beta=1e-0):\n",
    "    out = 0.5*F.l1_loss(y,yh, reduction = \"mean\")\n",
    "    out += gamma*F.mse_loss(y,yh, reduction = \"mean\")\n",
    "    out += alpha*torch.max(torch.abs(y-yh))\n",
    "    out += beta*(torch.sum(F.relu(-yh)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8ebd3f-905f-4f17-8a1f-ca2e30fb1300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m      2\u001b[0m                   PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m      3\u001b[0m                   optimizer \u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[1;32m      4\u001b[0m                   gamma_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.85\u001b[39m, \n\u001b[1;32m      5\u001b[0m                   step\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m, \n\u001b[1;32m      6\u001b[0m                   loss_function \u001b[38;5;241m=\u001b[39m linfty)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Scrivania/Tesi/leonardo/notebook/trainer.py:52\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m     50\u001b[0m     \n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# validate\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(val_loader)\n",
      "File \u001b[0;32m~/Scrivania/Tesi/leonardo/notebook/trainer.py:81\u001b[0m, in \u001b[0;36mTrainer._train\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(yh, y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/leonardo/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leonardo/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model = model.to(device), \n",
    "                  PATH = os.path.join(config['paths']['models'], \"testing.pt\"), \n",
    "                  optimizer = optimizer, \n",
    "                  gamma_scheduler = 0.85, \n",
    "                  step= 300, \n",
    "                  loss_function = linfty)\n",
    "\n",
    "trainer.fit(train_loader=dl_train, val_loader=dl_val, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db3198-91b0-4565-8454-130e4914a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, \n",
    "         config,\n",
    "         loss_training, \n",
    "         loss_validation, \n",
    "         name,\n",
    "         dl_train = dl_train,\n",
    "         dl_val = dl_val, \n",
    "         show = False):\n",
    "    fig = px.line({\"epochs\": range(1,len(loss_training)+1), \n",
    "                                   \"train\": loss_training, \n",
    "                                   \"validation\": loss_validation}, \n",
    "                                  x = \"epochs\", \n",
    "                                  y = [\"train\", \"validation\"], \n",
    "                                  title= f\"training loss for {name}\")\n",
    "    fig.add_vline(x = np.argsort(loss_validation)[0]+1)\n",
    "    fig.add_hline(y = np.min(loss_validation))\n",
    "    fig.write_html(os.path.join(config['paths']['fig'],'covid', f\"loss_gnn_{name}.html\"))\n",
    "    if show:\n",
    "        fig.show()\n",
    "    model = model.cpu()\n",
    "    model.device = torch.device('cpu')\n",
    "    if config['dataset']['aggregate']:\n",
    "        batch_train, y_train, adj_train = next(iter(dl_train))\n",
    "        batch_val, y_val, adj_val = next(iter(dl_val))\n",
    "        yh_train = model(batch_train.float().to(model.device), adj_train.to(model.device)).detach().numpy()\n",
    "        yh_val = model(batch_val.float().to(model.device), adj_val.to(model.device)).detach().numpy()\n",
    "    else:\n",
    "        yh_train = []\n",
    "        yh_val = []\n",
    "        y_train = []\n",
    "        y_val = []\n",
    "        for key in dl_train.keys():\n",
    "            batch_train, yt, adj_train = next(iter(dl_train[key]))\n",
    "            batch_val, yv, adj_val = next(iter(dl_val[key]))\n",
    "            yh_train.append(model(batch_train.float().to(model.device), adj_train.to(model.device)).cpu())\n",
    "            yh_val.append(model(batch_val.float().to(model.device), adj_val.to(model.device)).cpu())\n",
    "            y_train.append(yt)\n",
    "            y_val.append(yv)\n",
    "        \n",
    "        yh_train = torch.cat(yh_train, -2).transpose(-1,-2)\n",
    "        yh_val = torch.cat(yh_val, -2).transpose(-1,-2).detach().numpy()\n",
    "        y_train = torch.cat(y_train, -1).detach().numpy()\n",
    "        y_val = torch.cat(y_val, -1).detach().numpy()\n",
    "\n",
    "    lenght = 3*y.shape[1]\n",
    "    fig, ax = plt.subplots(nrows = y.shape[1], \n",
    "                           ncols = 2, \n",
    "                           constrained_layout = True,\n",
    "                           figsize = (20,lenght))\n",
    "    \n",
    "    for day in range(y.shape[1]):\n",
    "        ax[day, 0].plot(yh_train[0,day], label = \"estimate\")\n",
    "        ax[day, 0].plot(y_train[0,day], label =\"real\")\n",
    "    \n",
    "        ax[day, 1].plot(yh_val[0,day], label = \"estimate\")\n",
    "        ax[day, 1].plot(y_val[0,day], label =\"real\")\n",
    "        ax[day, 0].legend()\n",
    "        ax[day, 1].legend()\n",
    "    \n",
    "        ax[day, 0].title.set_text(f\"day {day +1} train\")\n",
    "        ax[day, 1].title.set_text(f\"day {day +1} validation\")\n",
    "    fig.suptitle(' Comparison between estimation and reality ', fontsize=20) \n",
    "    \n",
    "    path = os.path.join(config['paths']['fig'],'covid', f\"{name}.png\")\n",
    "    plt.savefig(path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4f5d1-6c33-4266-9905-7587bccadfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model,\n",
    "     config,\n",
    "     loss_training, \n",
    "     loss_validation, \n",
    "     f\"GLSTMseq2seq_{past_step}_{future_step}\", \n",
    "     show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17696f50-a174-4561-9f76-71bf32ce5b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
