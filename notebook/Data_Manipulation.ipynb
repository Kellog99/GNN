{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880f767c-063e-41ab-b717-159c314557f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89286644-478c-45ea-b4c4-51357958de33",
   "metadata": {},
   "source": [
    "Carico il configuratore che tiene traccia di tutte le informazioni generali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ea09f7-d49a-4aff-bcd2-e1bf7e5c97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "with open(\"./config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5028948-9440-480c-8007-9bb486239ecc",
   "metadata": {},
   "source": [
    "# Download Dati\n",
    "\n",
    "In questa parte carico i `.csv` e gli organizzo in modo tale che siano ordinati per\n",
    "\n",
    "`(data, denominazione_regionale, denominazione_provincia)`\n",
    "\n",
    "Inoltre pulisco i vari datase poiché ci sono una serie di righe che presentano delle problematicità in termini di valori.\n",
    "\n",
    "La matrice di adiacenza generale è stata creata connettendo tutte le provincie di una determinata regione, in modo tale da mantenere una similitudine con il problema originario. Inoltre, per non distaccarsi troppo con la realtà, sono state inserite delle connessioni tra regioni dato che quest'ultime non sono isolate. In particolare sono state aggiunte connessioni tra tutti i capoluoghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b263e7-92d5-41c3-b938-4feab58d4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(config: yaml):\n",
    "    if os.path.exists(os.path.join(config['paths']['adj'],\"adj_totale.pkl\")):\n",
    "        with open(os.path.join(config['paths']['adj'],\"adj_totale.pkl\"), \"rb\") as f:\n",
    "            adj = pickle.load(f)\n",
    "    else:\n",
    "        codice_reg_prov = pd.read_csv(os.path.join(config['paths']['data'], \"codice_reg_prov.csv\"), index_col=0)\n",
    "\n",
    "        # Carico un dizionario in cui le chiavi sono le regioni e i valori sono i rispettivi capoluohi\n",
    "        with open(os.path.join(config['paths']['data'], \"capoluoghi.json\"), 'r') as f:\n",
    "            capoluoghi = json.load(f)\n",
    "        \n",
    "        adjs = {}\n",
    "        adj = np.zeros((len(codice_reg_prov), len(codice_reg_prov)))\n",
    "        i = 0\n",
    "        index_capoluoghi = []\n",
    "        d = codice_reg_prov.groupby('denominazione_provincia').codice_provincia.max().to_dict()\n",
    "        regioni = codice_reg_prov.groupby(by=['codice_regione']).codice_provincia.unique()\n",
    "        \n",
    "        for cod_reg in tqdm(regioni.index):\n",
    "            codici_provincia = regioni[cod_reg]\n",
    "            n_prov = len(codici_provincia)\n",
    "            adjs[cod_reg] = np.ones((n_prov, n_prov))-np.eye(n_prov)\n",
    "            adj[i:i+n_prov, i:i+n_prov] = adjs[cod_reg]\n",
    "\n",
    "            region = codice_reg_prov[codice_reg_prov.codice_regione==cod_reg].denominazione_regione.values[0]\n",
    "            codice_capoluogo = codice_reg_prov[codice_reg_prov.denominazione_provincia==capoluoghi[region]].codice_provincia.values[0]\n",
    "            pos = np.where(codici_provincia == codice_capoluogo)[0][0]\n",
    "            index_capoluoghi.append(i+pos)\n",
    "            \n",
    "            \n",
    "            with open(os.path.join(config['paths']['adj'],f\"adj_{region}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(adj, f)\n",
    "                \n",
    "            i += n_prov\n",
    "        for ind_r in index_capoluoghi:\n",
    "            for ind_c in index_capoluoghi:\n",
    "                if ind_r != ind_c:\n",
    "                    adj[ind_r, ind_c] = 1\n",
    "                \n",
    "        with open(os.path.join(config['paths']['adj'],\"adj_totale.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(adj, f)\n",
    "        \n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1ab253-e1e1-447e-9627-ad5560fb24a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>variation_ricoverati_con_sintomi</th>\n",
       "      <th>variation_terapia_intensiva</th>\n",
       "      <th>variation_totale_ospedalizzati</th>\n",
       "      <th>variation_isolamento_domiciliare</th>\n",
       "      <th>variation_dimessi_guariti</th>\n",
       "      <th>variation_deceduti</th>\n",
       "      <th>totale_casi</th>\n",
       "      <th>nuovi_casi</th>\n",
       "      <th>ricoverati_con_sintomi</th>\n",
       "      <th>...</th>\n",
       "      <th>isolamento_domiciliare</th>\n",
       "      <th>dimessi_guariti</th>\n",
       "      <th>deceduti</th>\n",
       "      <th>codice_regione</th>\n",
       "      <th>codice_provincia</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>festivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data  variation_ricoverati_con_sintomi  variation_terapia_intensiva  \\\n",
       "0 2020-02-26                               0.0                          0.0   \n",
       "1 2020-02-26                               0.0                          0.0   \n",
       "2 2020-02-26                               0.0                          0.0   \n",
       "3 2020-02-26                               0.0                          0.0   \n",
       "4 2020-02-26                               0.0                          0.0   \n",
       "\n",
       "   variation_totale_ospedalizzati  variation_isolamento_domiciliare  \\\n",
       "0                             0.0                               0.0   \n",
       "1                             0.0                               0.0   \n",
       "2                             0.0                               0.0   \n",
       "3                             0.0                               0.0   \n",
       "4                             0.0                               0.0   \n",
       "\n",
       "   variation_dimessi_guariti  variation_deceduti  totale_casi  nuovi_casi  \\\n",
       "0                        0.0                 0.0          3.0         0.0   \n",
       "1                        0.0                 0.0          0.0         0.0   \n",
       "2                        0.0                 0.0          0.0         0.0   \n",
       "3                        0.0                 0.0          0.0         0.0   \n",
       "4                        0.0                 0.0          0.0         0.0   \n",
       "\n",
       "   ricoverati_con_sintomi  ...  isolamento_domiciliare  dimessi_guariti  \\\n",
       "0                     2.0  ...                     1.0              0.0   \n",
       "1                     2.0  ...                     1.0              0.0   \n",
       "2                     2.0  ...                     1.0              0.0   \n",
       "3                     2.0  ...                     1.0              0.0   \n",
       "4                     2.0  ...                     1.0              0.0   \n",
       "\n",
       "   deceduti  codice_regione  codice_provincia  year  month  day  day_of_week  \\\n",
       "0       0.0               1                 1     0      1   25            2   \n",
       "1       0.0               1                 2     0      1   25            2   \n",
       "2       0.0               1                 3     0      1   25            2   \n",
       "3       0.0               1                 4     0      1   25            2   \n",
       "4       0.0               1                 5     0      1   25            2   \n",
       "\n",
       "   festivo  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_dataset(df: pd, \n",
    "                 config:yaml):\n",
    "    # questa procedura serve solo ad inserire i dati sulle province mancanti\n",
    "    # questa procedura serve solo ad inserire i dati sulle province mancanti\n",
    "    d = []\n",
    "    feat = df.shape[1]\n",
    "    codici = list(df.codice_provincia.unique())\n",
    "    codice_reg_prov = {tuple(x) for x in df[config['dataset']['col_categorical_prov']].values.tolist()}\n",
    "    codice_reg_prov = pd.DataFrame(codice_reg_prov, columns=config['dataset']['col_categorical_prov'])\n",
    "    codice_reg_prov = codice_reg_prov.sort_values(by = ['codice_regione', 'codice_provincia']).reset_index(drop = True)\n",
    "    \n",
    "    codici = list(codice_reg_prov.codice_provincia.unique())\n",
    "    for data in tqdm(df.data.unique(), desc = \"filling\"):\n",
    "        tmp = df[df.data == data]\n",
    "        if len(tmp) != len(codici):\n",
    "            # individue le province mancanti\n",
    "            codici_mancanti = [x for x in codici if x not in tmp.codice_provincia.unique()]\n",
    "            tmp = pd.concat([tmp, codice_reg_prov[codice_reg_prov.codice_provincia.isin(codici_mancanti)]])        \n",
    "            tmp.data = data\n",
    "            tmp = tmp.fillna(0)\n",
    "    \n",
    "            if len(tmp) != len(codici):\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "        d.append(tmp)\n",
    "    df = pd.concat(d).sort_values(by = ['data','codice_regione', 'codice_provincia']).reset_index(drop = True)\n",
    "    df = df[list(df.columns.drop(['codice_regione', 'codice_provincia']))+['codice_regione', 'codice_provincia']]\n",
    "    return df\n",
    "\n",
    "def get_dataset_regioni(config:yaml):\n",
    "    if os.path.exists(os.path.join(config['paths']['data'], \"covid_regioni.csv\")):\n",
    "            regions = pd.read_csv(os.path.join(config['paths']['data'],\"covid_regioni.csv\"), index_col=0)\n",
    "            regions['data'] = pd.to_datetime(regions['data'], format='%Y-%m-%d')\n",
    "    else:\n",
    "        ########## I create take the dataset for the regions\n",
    "        data = []\n",
    "        for csv in tqdm(os.listdir(os.path.join(config['paths']['raw_data'],\"dati-regioni\")), desc = \"regione\"):\n",
    "            if csv.split(\".\")[-1] == \"csv\":\n",
    "                data.append(pd.read_csv(os.path.join(config['paths']['raw_data'],\"dati-regioni\",csv)))\n",
    "        data = pd.concat(data).drop_duplicates().reset_index(drop = True)\n",
    "        \n",
    "        # Trento\n",
    "        data.codice_regione = data.codice_regione.replace(21, 4)\n",
    "        # Bolzano\n",
    "        data.codice_regione = data.codice_regione.replace(22, 4)\n",
    "        # in questo modo sommo tutte le variabili delle due provincie\n",
    "        # non ci sono problemi perché le variabili rimanenti sono solo numeriche \n",
    "        data = data.groupby(by = ['data', 'codice_regione']).sum().reset_index()\n",
    "        \n",
    "        regions = data[['data', 'codice_regione', 'ricoverati_con_sintomi', 'terapia_intensiva', 'totale_ospedalizzati',\n",
    "           'isolamento_domiciliare', 'dimessi_guariti', 'deceduti']]\n",
    "        regions.data = regions.data.apply(lambda x: x.split(\"T\")[0])\n",
    "        regions.data = pd.to_datetime(regions['data'], format='%Y-%m-%d')\n",
    "        \n",
    "        regions = regions.drop_duplicates().reset_index(drop = True)\n",
    "        regions.to_csv(os.path.join(config['paths']['data'],\"covid_regioni.csv\"))\n",
    "    return regions\n",
    "def get_dataset_province(config:yaml):\n",
    "    if os.path.exists(os.path.join(config['paths']['data'],\"covid_province.csv\")):\n",
    "            provincials = pd.read_csv(os.path.join(config['paths']['data'],\"covid_province.csv\"), index_col = 0)\n",
    "            provincials['data'] = pd.to_datetime(provincials['data'], format='%Y-%m-%d')\n",
    "    else:\n",
    "        data = []\n",
    "        for csv in tqdm(os.listdir(os.path.join(config['paths']['raw_data'],\"dati-province\")), desc = \"provincia\"):\n",
    "            if csv.split(\".\")[-1] == \"csv\":\n",
    "                tmp = pd.read_csv(os.path.join(config['paths']['raw_data'],\"dati-province\",csv), \n",
    "                                  usecols = ['data', 'codice_regione', 'codice_provincia', 'totale_casi'])\n",
    "                data.append(tmp)\n",
    "        \n",
    "        data = pd.concat(data)\n",
    "        data.reset_index(drop = True, inplace=True)\n",
    "        data.data = data.data.apply(lambda x: x.split(\"T\")[0])\n",
    "        data.data = pd.to_datetime(data['data'], format='%Y-%m-%d')\n",
    "        \n",
    "        # Riduco il dataset con le variabili che servono\n",
    "        # Inoltre trasformo la conta totale dei positivi per ogni regione in nuovi positivi\n",
    "        #data.rename(columns={'totale_casi': 'nuovi_casi'}, inplace=True)\n",
    "        provincials = data.drop_duplicates()\n",
    "        provincials = provincials[-(provincials.codice_provincia>200)]\n",
    "        \n",
    "        # codici relative alle province che sono state male elaborate\n",
    "        provincials = provincials[-provincials.codice_regione.isin([21,22])]\n",
    "        \n",
    "        ############## creo la variazione giornaliera dei contagi  ##########################à\n",
    "        provincials = provincials[['data','codice_regione','codice_provincia', 'totale_casi']].sort_values(by = ['data', 'codice_provincia'])\n",
    "        provincials['nuovi_casi']= provincials.groupby('codice_provincia').diff().totale_casi\n",
    "        provincials = provincials.dropna().reset_index(drop = True)\n",
    "        # Siccome ci sono giorni in cui il numero di positivi è negativo per via di errori\n",
    "        # segno quei valori a zero e quindi il totale positivi rimane invariato \n",
    "        print(\"hello\")\n",
    "        provincials[provincials.nuovi_casi<0].totale_casi = provincials.loc[list(provincials[provincials.nuovi_casi<0].index-1)].totale_casi\n",
    "        provincials.nuovi_casi = provincials.nuovi_casi.apply(lambda x: max(0, x))\n",
    "        provincials.to_csv(os.path.join(config['paths']['data'],\"covid_province.csv\"))\n",
    "    \n",
    "    return provincials\n",
    "    \n",
    "def get_dataset_covid(config: yaml):\n",
    "    # Creo il dataset completo \n",
    "    if os.path.exists(os.path.join(config['paths']['data'],\"covid.csv\")):\n",
    "        df = pd.read_csv(os.path.join(config['paths']['data'], \"covid.csv\"), index_col=0)\n",
    "        df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
    "\n",
    "    else:\n",
    "        provincials = get_dataset_province(config)\n",
    "        regions = get_dataset_regioni(config)\n",
    "        df = pd.merge(provincials, regions, \n",
    "                      how = \"inner\",  \n",
    "                      on = ['data','codice_regione']) \n",
    "        df = df.sort_values(by = ['data','codice_regione', 'codice_provincia']).reset_index(drop = True)\n",
    "        codice_reg_prov = {tuple(x) for x in df[config['dataset']['col_categorical_prov']].values.tolist()}\n",
    "        codice_reg_prov = pd.DataFrame(codice_reg_prov, columns=config['dataset']['col_categorical_prov'])\n",
    "\n",
    "        ################# Filling the dataset ###################\n",
    "        df = fill_dataset(df, config)\n",
    "\n",
    "        ################# Date variables ########################\n",
    "        df['year'] = df.data.dt.year-df.data.dt.year.min()\n",
    "        df['month'] = df.data.dt.month-df.data.dt.month.min()\n",
    "        df['day'] = df.data.dt.day-df.data.dt.day.min()\n",
    "        df['day_of_week'] = df.data.dt.dayofweek\n",
    "        df['festivo'] = 0\n",
    "        df[df.day_of_week>5].festivo = 1\n",
    "\n",
    "        ################# variation ##########\n",
    "        num_variables = df.columns[3:-7]\n",
    "        tmp=df.groupby('codice_provincia').diff()[num_variables]\n",
    "        tmp.columns = [f\"variation_{x}\" for x in tmp.columns]\n",
    "        df = pd.concat([df.data, tmp, df.drop(columns = 'data')], 1)\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        codice_reg_prov.to_csv(os.path.join(config['paths']['data'],\"codice_reg_prov.csv\"))\n",
    "        df.to_csv(os.path.join(config['paths']['data'],\"covid.csv\"))\n",
    "    return df\n",
    "    \n",
    "data = get_dataset_covid(config)\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
