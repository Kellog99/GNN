{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880f767c-063e-41ab-b717-159c314557f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89286644-478c-45ea-b4c4-51357958de33",
   "metadata": {},
   "source": [
    "Carico il configuratore che tiene traccia di tutte le informazioni generali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ea09f7-d49a-4aff-bcd2-e1bf7e5c97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "with open(\"./config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5028948-9440-480c-8007-9bb486239ecc",
   "metadata": {},
   "source": [
    "# Download Dati\n",
    "\n",
    "In questa parte carico i `.csv` e gli organizzo in modo tale che siano ordinati per\n",
    "\n",
    "`(data, denominazione_regionale, denominazione_provincia)`\n",
    "\n",
    "Inoltre pulisco i vari datase poiché ci sono una serie di righe che presentano delle problematicità in termini di valori.\n",
    "\n",
    "La matrice di adiacenza generale è stata creata connettendo tutte le provincie di una determinata regione, in modo tale da mantenere una similitudine con il problema originario. Inoltre, per non distaccarsi troppo con la realtà, sono state inserite delle connessioni tra regioni dato che quest'ultime non sono isolate. In particolare sono state aggiunte connessioni tra tutti i capoluoghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b263e7-92d5-41c3-b938-4feab58d4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(config: yaml):\n",
    "    if os.path.exists(os.path.join(config['paths']['adj'],\"adj_totale.pkl\")):\n",
    "        with open(os.path.join(config['paths']['adj'],\"adj_totale.pkl\"), \"rb\") as f:\n",
    "            adj = pickle.load(f)\n",
    "    else:\n",
    "        codice_reg_prov = pd.read_csv(os.path.join(config['paths']['data'], \"codice_reg_prov.csv\"), index_col=0)\n",
    "\n",
    "        # Carico un dizionario in cui le chiavi sono le regioni e i valori sono i rispettivi capoluohi\n",
    "        with open(os.path.join(config['paths']['data'], \"capoluoghi.json\"), 'r') as f:\n",
    "            capoluoghi = json.load(f)\n",
    "        \n",
    "        adjs = {}\n",
    "        adj = np.zeros((len(codice_reg_prov), len(codice_reg_prov)))\n",
    "        i = 0\n",
    "        index_capoluoghi = []\n",
    "        d = codice_reg_prov.groupby('denominazione_provincia').codice_provincia.max().to_dict()\n",
    "        regioni = codice_reg_prov.groupby(by=['codice_regione']).codice_provincia.unique()\n",
    "        \n",
    "        for cod_reg in tqdm(regioni.index):\n",
    "            codici_provincia = regioni[cod_reg]\n",
    "            n_prov = len(codici_provincia)\n",
    "            adjs[cod_reg] = np.ones((n_prov, n_prov))-np.eye(n_prov)\n",
    "            adj[i:i+n_prov, i:i+n_prov] = adjs[cod_reg]\n",
    "\n",
    "            region = codice_reg_prov[codice_reg_prov.codice_regione==cod_reg].denominazione_regione.values[0]\n",
    "            codice_capoluogo = codice_reg_prov[codice_reg_prov.denominazione_provincia==capoluoghi[region]].codice_provincia.values[0]\n",
    "            pos = np.where(codici_provincia == codice_capoluogo)[0][0]\n",
    "            index_capoluoghi.append(i+pos)\n",
    "            \n",
    "            \n",
    "            with open(os.path.join(config['paths']['adj'],f\"adj_{region}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(adj, f)\n",
    "                \n",
    "            i += n_prov\n",
    "        for ind_r in index_capoluoghi:\n",
    "            for ind_c in index_capoluoghi:\n",
    "                if ind_r != ind_c:\n",
    "                    adj[ind_r, ind_c] = 1\n",
    "                \n",
    "        with open(os.path.join(config['paths']['adj'],\"adj_totale.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(adj, f)\n",
    "        \n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1ab253-e1e1-447e-9627-ad5560fb24a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fill_dataset(df: pd, \n",
    "                 config:yaml):\n",
    "    # questa procedura serve solo ad inserire i dati sulle province mancanti\n",
    "    # questa procedura serve solo ad inserire i dati sulle province mancanti\n",
    "    d = []\n",
    "    feat = df.shape[1]\n",
    "    codici = list(df.codice_provincia.unique())\n",
    "    codice_reg_prov = {tuple(x) for x in df[config['dataset']['col_categorical_prov']].values.tolist()}\n",
    "    codice_reg_prov = pd.DataFrame(codice_reg_prov, columns=config['dataset']['col_categorical_prov'])\n",
    "    codice_reg_prov = codice_reg_prov.sort_values(by = ['codice_regione', 'codice_provincia']).reset_index(drop = True)\n",
    "    \n",
    "    codici = list(codice_reg_prov.codice_provincia.unique())\n",
    "    for data in tqdm(df.data.unique(), desc = \"filling\"):\n",
    "        tmp = df[df.data == data]\n",
    "        if len(tmp) != len(codici):\n",
    "            # individue le province mancanti\n",
    "            codici_mancanti = [x for x in codici if x not in tmp.codice_provincia.unique()]\n",
    "            tmp = pd.concat([tmp, codice_reg_prov[codice_reg_prov.codice_provincia.isin(codici_mancanti)]])        \n",
    "            tmp.data = data\n",
    "            tmp = tmp.fillna(0)\n",
    "    \n",
    "            if len(tmp) != len(codici):\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "        d.append(tmp)\n",
    "    df = pd.concat(d).sort_values(by = ['data','codice_regione', 'codice_provincia']).reset_index(drop = True)\n",
    "    df = df[list(df.columns.drop(['codice_regione', 'codice_provincia']))+['codice_regione', 'codice_provincia']]\n",
    "    return df\n",
    "    \n",
    "def get_dataset(config: yaml):\n",
    "    files = [\"covid\", \"covid_province\", \"covid_regioni\"]\n",
    "    exists = [os.path.exists(os.path.join(config['paths']['data'], f\"{file}.csv\")) for file in files]\n",
    "\n",
    "\n",
    "    ############## Checking if all the files that I need exists #######################\n",
    "    if  np.all(exists):\n",
    "        tmp = pd.read_csv(os.path.join(config['paths']['data'], f\"covid.csv\"), index_col=0)\n",
    "        tmp.data = pd.to_datetime(tmp.data, format='%Y-%m-%d')\n",
    "        return tmp\n",
    "        \n",
    "    else:\n",
    "        ########## I create take the dataset for the provinces\n",
    "        if os.path.exists(os.path.join(config['paths']['data'],\"covid_province.csv\")):\n",
    "            provincials = pd.read_csv(os.path.join(config['paths']['data'],\"covid_province.csv\"), index_col = 0)\n",
    "            provincials['data'] = pd.to_datetime(provincials['data'], format='%Y-%m-%d')\n",
    "        else:\n",
    "            data = []\n",
    "            for csv in tqdm(os.listdir(os.path.join(config['paths']['raw_data'],\"dati-province\")), desc = \"provincia\"):\n",
    "                if csv.split(\".\")[-1] == \"csv\":\n",
    "                    tmp = pd.read_csv(os.path.join(config['paths']['raw_data'],\"dati-province\",csv), \n",
    "                                      usecols = ['data', 'codice_regione', 'codice_provincia', 'totale_casi'])\n",
    "                    data.append(tmp)\n",
    "            \n",
    "            data = pd.concat(data)\n",
    "            data.reset_index(drop = True, inplace=True)\n",
    "            data.data = data.data.apply(lambda x: x.split(\"T\")[0])\n",
    "            data.data = pd.to_datetime(data['data'], format='%Y-%m-%d')\n",
    "            \n",
    "            # Riduco il dataset con le variabili che servono\n",
    "            # Inoltre trasformo la conta totale dei positivi per ogni regione in nuovi positivi\n",
    "            data.rename(columns={'totale_casi': 'nuovi_casi'}, inplace=True)\n",
    "            provincials = data.drop_duplicates()\n",
    "            provincials = provincials[-(provincials.codice_provincia>200)]\n",
    "            \n",
    "            # codici relative alle province che sono state male elaborate\n",
    "            provincials = provincials[-provincials.codice_regione.isin([21,22])]\n",
    "            \n",
    "            ############## creo la variazione giornaliera dei contagi  ##########################à\n",
    "            provincials = provincials[['data','codice_regione','codice_provincia', 'nuovi_casi']]\n",
    "            d = []\n",
    "            for id in provincials.codice_provincia.unique():\n",
    "                tmp = provincials[provincials.codice_provincia == id].sort_values(by='data')\n",
    "                tmp[['diff_data', 'nuovi_casi']] = tmp[['data', 'nuovi_casi']].diff()\n",
    "                d.append(tmp.dropna())\n",
    "            tmp = pd.concat(d, ignore_index=True)\n",
    "            provincials = tmp[tmp.diff_data == np.timedelta64(1, 'D')].drop(columns = \"diff_data\").sort_values(by = ['data', 'codice_provincia']).reset_index(drop=True)\n",
    "            provincials.to_csv(os.path.join(config['paths']['data'],\"covid_province.csv\"))\n",
    "        \n",
    "        if os.path.exists(os.path.join(config['paths']['data'], \"covid_regioni.csv\")):\n",
    "            regions = pd.read_csv(os.path.join(config['paths']['data'],\"covid_regioni.csv\"), index_col=0)\n",
    "            regions['data'] = pd.to_datetime(regions['data'], format='%Y-%m-%d')\n",
    "        else:\n",
    "            ########## I create take the dataset for the regions\n",
    "            data = []\n",
    "            for csv in tqdm(os.listdir(os.path.join(config['paths']['raw_data'],\"dati-regioni\")), desc = \"regione\"):\n",
    "                if csv.split(\".\")[-1] == \"csv\":\n",
    "                    data.append(pd.read_csv(os.path.join(config['paths']['raw_data'],\"dati-regioni\",csv)))\n",
    "            data = pd.concat(data).drop_duplicates().reset_index(drop = True)\n",
    "            \n",
    "            # Trento\n",
    "            data.codice_regione = data.codice_regione.replace(21, 4)\n",
    "            # Bolzano\n",
    "            data.codice_regione = data.codice_regione.replace(22, 4)\n",
    "            # in questo modo sommo tutte le variabili delle due provincie\n",
    "            # non ci sono problemi perché le variabili rimanenti sono solo numeriche \n",
    "            data = data.groupby(by = ['data', 'codice_regione']).sum().reset_index()\n",
    "            \n",
    "            regions = data[['data', 'codice_regione', 'ricoverati_con_sintomi', 'terapia_intensiva', 'totale_ospedalizzati',\n",
    "               'isolamento_domiciliare', 'dimessi_guariti', 'deceduti']]\n",
    "            regions.data = regions.data.apply(lambda x: x.split(\"T\")[0])\n",
    "            regions.data = pd.to_datetime(regions['data'], format='%Y-%m-%d')\n",
    "            \n",
    "            regions = regions.drop_duplicates().reset_index(drop = True)\n",
    "            regions.to_csv(os.path.join(config['paths']['data'],\"covid_regioni.csv\"))\n",
    "                    \n",
    "        # Creo il dataset completo \n",
    "        if os.path.exists(os.path.join(config['paths']['data'],\"covid.csv\")):\n",
    "            df = pd.read_csv(os.path.join(config['paths']['data'], \"covid.csv\"), index_col=0)\n",
    "            df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
    "\n",
    "        else:\n",
    "            df = pd.merge(provincials, regions, \n",
    "                          how = \"inner\",  \n",
    "                          on = ['data','codice_regione']) \n",
    "            df = df.sort_values(by = ['data','codice_regione', 'codice_provincia']).reset_index(drop = True)\n",
    "            \n",
    "            #df = df[config['dataset']['col_data'] + config['dataset']['col_numerical_prov']+config['dataset']['col_numerical_reg']+config['dataset']['col_categorical_prov']]\n",
    "\n",
    "            codice_reg_prov = {tuple(x) for x in df[config['dataset']['col_categorical_prov']].values.tolist()}\n",
    "            codice_reg_prov = pd.DataFrame(codice_reg_prov, columns=config['dataset']['col_categorical_prov'])\n",
    "\n",
    "            ################# Filling the dataset ###################\n",
    "            df = fill_dataset(df, config)\n",
    "\n",
    "            ################# Date variables ########################\n",
    "            df['year'] = df.data.dt.year-df.data.dt.year.min()\n",
    "            df['month'] = df.data.dt.month-df.data.dt.month.min()\n",
    "            df['day'] = df.data.dt.day-df.data.dt.day.min()\n",
    "            df['day_of_week'] = df.data.dt.dayofweek\n",
    "            df['festivo'] = 0\n",
    "            df[df.day_of_week>5].festivo = 1\n",
    "            \n",
    "            ################ Variation variables ####################\n",
    "            new_column = ['data']\n",
    "            num_col = df.columns[1:-5]\n",
    "            for num in num_col:\n",
    "                new_column.append(f\"daily_change_{num}\")\n",
    "                df[new_column[-1]]=df.groupby('codice_provincia')[num].diff()\n",
    "            df = pd.concat([df[new_column], df.drop(new_column, axis=1)], axis=1)\n",
    "            df.dropna(inplace = True)\n",
    "            df.reset_index(drop = True, inplace= True)\n",
    "\n",
    "            \n",
    "            ################# Saving the dataset #####################\n",
    "            codice_reg_prov.to_csv(os.path.join(config['paths']['data'],\"codice_reg_prov.csv\"))\n",
    "            df.to_csv(os.path.join(config['paths']['data'],\"covid.csv\"))\n",
    "            \n",
    "    return df\n",
    "    \n",
    "data = get_dataset(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9210b637-fb59-4f98-afa6-d5e3339cd360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>daily_change_nuovi_casi</th>\n",
       "      <th>daily_change_ricoverati_con_sintomi</th>\n",
       "      <th>daily_change_terapia_intensiva</th>\n",
       "      <th>daily_change_totale_ospedalizzati</th>\n",
       "      <th>daily_change_isolamento_domiciliare</th>\n",
       "      <th>daily_change_dimessi_guariti</th>\n",
       "      <th>daily_change_deceduti</th>\n",
       "      <th>daily_change_codice_regione</th>\n",
       "      <th>daily_change_codice_provincia</th>\n",
       "      <th>...</th>\n",
       "      <th>isolamento_domiciliare</th>\n",
       "      <th>dimessi_guariti</th>\n",
       "      <th>deceduti</th>\n",
       "      <th>codice_regione</th>\n",
       "      <th>codice_provincia</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>festivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data  daily_change_nuovi_casi  daily_change_ricoverati_con_sintomi  \\\n",
       "0 2020-02-26                     -3.0                                  0.0   \n",
       "1 2020-02-26                      0.0                                  0.0   \n",
       "2 2020-02-26                      0.0                                  0.0   \n",
       "3 2020-02-26                      0.0                                  0.0   \n",
       "4 2020-02-26                      0.0                                  0.0   \n",
       "\n",
       "   daily_change_terapia_intensiva  daily_change_totale_ospedalizzati  \\\n",
       "0                             0.0                                0.0   \n",
       "1                             0.0                                0.0   \n",
       "2                             0.0                                0.0   \n",
       "3                             0.0                                0.0   \n",
       "4                             0.0                                0.0   \n",
       "\n",
       "   daily_change_isolamento_domiciliare  daily_change_dimessi_guariti  \\\n",
       "0                                  0.0                           0.0   \n",
       "1                                  0.0                           0.0   \n",
       "2                                  0.0                           0.0   \n",
       "3                                  0.0                           0.0   \n",
       "4                                  0.0                           0.0   \n",
       "\n",
       "   daily_change_deceduti  daily_change_codice_regione  \\\n",
       "0                    0.0                          0.0   \n",
       "1                    0.0                          0.0   \n",
       "2                    0.0                          0.0   \n",
       "3                    0.0                          0.0   \n",
       "4                    0.0                          0.0   \n",
       "\n",
       "   daily_change_codice_provincia  ...  isolamento_domiciliare  \\\n",
       "0                            0.0  ...                     1.0   \n",
       "1                            0.0  ...                     1.0   \n",
       "2                            0.0  ...                     1.0   \n",
       "3                            0.0  ...                     1.0   \n",
       "4                            0.0  ...                     1.0   \n",
       "\n",
       "   dimessi_guariti  deceduti  codice_regione  codice_provincia  year  month  \\\n",
       "0              0.0       0.0               1                 1     0      1   \n",
       "1              0.0       0.0               1                 2     0      1   \n",
       "2              0.0       0.0               1                 3     0      1   \n",
       "3              0.0       0.0               1                 4     0      1   \n",
       "4              0.0       0.0               1                 5     0      1   \n",
       "\n",
       "   day  day_of_week  festivo  \n",
       "0   25            2        0  \n",
       "1   25            2        0  \n",
       "2   25            2        0  \n",
       "3   25            2        0  \n",
       "4   25            2        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6783c20f-66ef-4fda-af0e-5ed1a35763f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data', 'daily_change_nuovi_casi',\n",
       "       'daily_change_ricoverati_con_sintomi', 'daily_change_terapia_intensiva',\n",
       "       'daily_change_totale_ospedalizzati',\n",
       "       'daily_change_isolamento_domiciliare', 'daily_change_dimessi_guariti',\n",
       "       'daily_change_deceduti', 'daily_change_codice_regione',\n",
       "       'daily_change_codice_provincia', 'nuovi_casi', 'ricoverati_con_sintomi',\n",
       "       'terapia_intensiva', 'totale_ospedalizzati', 'isolamento_domiciliare',\n",
       "       'dimessi_guariti', 'deceduti', 'codice_regione', 'codice_provincia',\n",
       "       'year', 'month', 'day', 'day_of_week', 'festivo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a00906-de9d-45df-a69d-f014f78a8570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
